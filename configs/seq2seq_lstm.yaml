model_name: seq2seq_lstm

base_model:
  enc_emb_dim: 256
  enc_hid_dim: 512
  enc_dropout: 0.5
  dec_emb_dim: 256
  dec_hid_dim: 512
  dec_dropout: 0.5
  n_layers: 2

general:
  logs_dir: logs

# configure optimization
optimization:
  loss:
    name: 'bce'

  optimizer:
    path: torch.optim.AdamW
    params:
      lr: 0.0003
      betas: !!python/tuple [0.5, 0.999]
      weight_decay: 0.01

  scheduler:
    path: torch.optim.lr_scheduler.ReduceLROnPlateau
    monitor: val_loss
    step_size: 10
    gamma: 0.2
    interval: epoch
    frequency: 1
    strict: True
    params:
      factor: 0.5
      patience: 7
      min_lr: 1.0e-5
      verbose: true

checkpoint_callback:
  mode: min
  monitor: val_loss
  save_top_k: 3
  verbose: True

trainer:
  gpus: null #1'
  distributed_backend: null
  accumulate_grad_batches: 1
  profiler: False
  max_epochs: 2
  flush_logs_every_n_steps: 10
  #  log_save_interval: 1
  gradient_clip_val: 0.5
  num_sanity_val_steps: -1
  check_val_every_n_epoch: 1

seed: 13

data:
  batch_size: 128
  epoch_size: 10

  with_pad: False
  inversion_dataset: True
  batch_first: False
  clip: 1
